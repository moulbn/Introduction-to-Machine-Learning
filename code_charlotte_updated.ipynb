{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj_hDMvpzztA"
   },
   "source": [
    "<center>\n",
    "<a href=\"https://www.tse-fr.eu/\" ><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/52/TSE_Logo_2019.png\" style=\"float:left; width: \"200\"; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"float:right; max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "kCLoSIVizztH"
   },
   "source": [
    "# [Machine learning scenario](https://github.com/wikistat/Apprentissage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlU6SOZezztK"
   },
   "source": [
    "# GRC: Prediction of a bank product appetancy with <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a> and <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 100px; display: inline\" alt=\"Scikit-learn\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2tjy71BzztM"
   },
   "source": [
    "#### Summary\n",
    "The dataset is built with 825 customers of a bank, described by 32 variables. These variables give some informations about the bank use. Our goal is to predict a score for the Visa Premier card: we intend to compare several methods of machine learning (logistic regression, trees, extreme gradient boosting, svm or random forests).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Data presentation\n",
    "#### Variables\n",
    "The variables are listed below. \n",
    "*Table: name and meaning of variables* Beware, some variables are written in capital letters and then in small letters after some transformations (logarithm, one hot encoding, etc). Names ending with L correspond to variables that are logarithmically transformed while names ending with Q or q correspond to qualitative variables. . \n",
    "\n",
    "**Identifiant** | **Libellé**\n",
    "           --|--\n",
    "`sexeq` | Sexe (qualitatif) \n",
    "`ager` | Age en années\n",
    "`famiq` | Situation familiale: `Fmar Fcel Fdiv Fuli Fsep Fveu`\n",
    "`relat` | Ancienneté de relation en mois\n",
    "`pcspq` | Catégorie socio-professionnelle (code num)\n",
    "`opgnb` | Nombre d'opérations par guichet dans le mois\n",
    "`moyrv` | Moyenne des mouvements nets créditeurs des 3 mois en Kf\n",
    "`tavep` | Total des avoirs épargne monétaire en francs\n",
    "`endet` | Taux d'endettement\n",
    "`gaget` | Total des engagements en francs\n",
    "`gagec` | Total des engagements court terme en francs\n",
    "`gagem` | Total des engagements moyen terme en francs\n",
    "`kvunb` | Nombre de comptes à vue\n",
    "`qsmoy` | Moyenne des soldes moyens sur 3 mois\n",
    "`qcred` | Moyenne des mouvements créditeurs en Kf\n",
    "`dmvtp` | Age du dernier mouvement (en jours)\\hline\n",
    "`boppn` | Nombre d'opérations à M-1\n",
    "`facan` | Montant facturé dans l'année en francs\n",
    "`lgagt` | Engagement long terme\n",
    "`vienb` | Nombre de produits contrats vie\n",
    "`viemt` | Montant des produits contrats vie en francs\n",
    "`uemnb` | Nombre de produits épargne monétaire\n",
    "`xlgnb` | Nombre de produits d'épargne logement\n",
    "`xlgmt` | Montant des produits d'épargne logement en francs\n",
    "`ylvnb` | Nombre de comptes sur livret\n",
    "`ylvmt` | Montant des comptes sur livret en francs\n",
    "`rocnb` | Nombre de paiements par carte bancaire à M-1\n",
    "`nptag` | Nombre de cartes point argent\n",
    "`itavc` | Total des avoirs sur tous les comptes\n",
    "`havef` | Total des avoirs épargne financière en francs\n",
    "`jnbjd | Nombre de jours à débit à M\n",
    "**`carvp`** | **Possession de la carte VISA Premier**\n",
    "\n",
    "\n",
    "**Answer the questions with the help of Python and Scikitlearn.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) \n",
    "The Bayes Rule states that \n",
    "\n",
    "for $(X_i, Y_i)_{1 \\leq i \\leq N})$ such that $X \\in R^P and Y \\in R$\n",
    "\n",
    "a training set $D_n = { (X_1, Y_1), ..., (X_n, Y_n)} i.i.d according to $P$\n",
    "\n",
    "$ the best solution $f*$, predictor function in $F = { X(\\Omega) --> Y(\\Omega) a.s } $ equals\n",
    "\n",
    "\n",
    "$ f* = argmin \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQ9PtbSBzztP"
   },
   "source": [
    "## Data preparation\n",
    "### Lecture \n",
    "The original dataset has been transformed after an important preprocessing step in this [notebook](https://github.com/wikistat/Exploration/blob/master/GRC-carte_Visa/Explo-R-Visa.ipynb). We skip this step today! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HLRhAQ5HzztQ"
   },
   "outputs": [],
   "source": [
    "# Library Importation\n",
    "import numpy as np\n",
    "import socket\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "BRSf5g7nzztT",
    "outputId": "2aff5a27-6a2c-4b54-a3ae-205741b1a8b9"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Charlotte/Desktop/TSE/M2/MATHS OF DEEP LEARNING/DATA/vispremv.dat.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6536/3574534824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#files.upload()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mvispremv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'C:/Users/Charlotte/Desktop/TSE/M2/MATHS OF DEEP LEARNING/DATA/vispremv.dat.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#Give the size of the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\math4ml\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\math4ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_table\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, encoding_errors, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    681\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\math4ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\math4ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\math4ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\math4ml\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\math4ml\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\math4ml\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Charlotte/Desktop/TSE/M2/MATHS OF DEEP LEARNING/DATA/vispremv.dat.txt'"
     ]
    }
   ],
   "source": [
    "# Lecture of the data frame\n",
    "#files.upload()\n",
    "\n",
    "vispremv = pd.read_table( 'C:/Users/Charlotte/Desktop/TSE/M2/MATHS OF DEEP LEARNING/DATA/vispremv.dat.txt', delimiter = ' ' )\n",
    "#Give the size of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "R8_7Y_mgzztU",
    "outputId": "e63b65de-d7cd-40d5-8bc5-2d0780238dd8"
   },
   "outputs": [],
   "source": [
    "vispremv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vispremv = vispremv.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "xRlqVxvrzztX",
    "outputId": "1814b555-1b28-492d-f751-324071c7f3a9"
   },
   "outputs": [],
   "source": [
    "# Produce a list of quantitative variables\n",
    "vispremv.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj4YpUKuzztY"
   },
   "source": [
    "Verify that most of the variables are described by a quantitative and a qualitative version.\n",
    "\n",
    "Qualitative variables  (sexe, csp, famille)  are transformed into quantitative ones except `CARVP`. How? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skQOxgwEzztY",
    "outputId": "439e26ba-c245-48b1-d150-f5cbc7c49a42"
   },
   "outputs": [],
   "source": [
    "vispremv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jIVGUUXDBQ-"
   },
   "outputs": [],
   "source": [
    "vispremv = vispremv.loc[:,~vispremv.columns.duplicated() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xa1IRj-4zztZ",
    "outputId": "52854a1e-f8f0-42ef-f48b-a4a8481727c9"
   },
   "outputs": [],
   "source": [
    "# Transformation of qualitative variables into quantitative 0/1 ones.\n",
    "# Use pd.get_dummies\n",
    "\n",
    "vispremDum = pd.get_dummies( vispremv[['SEXEQ', 'FAMIQ', 'PCSPQ']] ) \n",
    "vispremDum\n",
    "\n",
    "vispremDum.drop( ['SEXEQ_Sfem', 'FAMIQ_Fseu'], axis = 1, inplace = True )\n",
    "\n",
    "vispremNum = vispremDum.select_dtypes( exclude = ['object'] )\n",
    "\n",
    "# Pre processing: Scaling \n",
    "\n",
    "\n",
    "# Follows Standard Normal Distribution (the mean is not a statistic that is robust to outliers, the median is)\n",
    "normalized_df = ( vispremNum - vispremNum.mean() )/ vispremNum.std()\n",
    "\n",
    "# Scales in the range [0,1] for positives real values or else in [-1,1] for negative real values.\n",
    "# All inliers are compressed in [0, 0.005]. It is not robust to outliers \n",
    "min_max_df = ( vispremNum - vispremNum.min() ) /( vispremNum.max() - vispremNum.min() )\n",
    "\n",
    "# Robust Scaler scales statistics that are robust to outliers: \n",
    "# it removes the median and scales the data in the range between the 1st and 3rd quartiles or the \n",
    "# interquartile range.\n",
    "\n",
    "robust_df = ( vispremNum - vispremNum.median() ) / ( vispremNum.quantile( 0.25 ) - vispremNum.quantile( 0.75 ) )\n",
    "\n",
    "vispremR = pd.concat( [vispremDum, robust_df], axis = 1 )\n",
    "\n",
    "print( vispremR.columns )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrxrIdPHPTQs",
    "outputId": "615ce2d3-40ed-4334-8e3a-90aa26deed1d"
   },
   "outputs": [],
   "source": [
    "vispremR.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vispremR = vispremR.loc[:,~vispremR.columns.duplicated() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vispremR.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA5DcEW8zzta"
   },
   "source": [
    "** **texte en gras**Q** How many samples and how many variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMMDekUAzzta",
    "outputId": "04eb0c91-15ce-4395-db3b-ce2d4cd2ce9b"
   },
   "outputs": [],
   "source": [
    "#TODO Compute the number of samples and the number of variables\n",
    "\n",
    "vispremR.shape\n",
    "\n",
    "#There are 1063 samples and 7 variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibjHCLbPzztb"
   },
   "outputs": [],
   "source": [
    "# The variable CARVP is modified into 0/1\n",
    "y = vispremv[\"CARVP\"].map(lambda x: 0 if x==\"Cnon\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsDwsm8uqqPI"
   },
   "outputs": [],
   "source": [
    "vispremv[\"CARVP\"] = vispremv[\"CARVP\"].map(lambda x: 0 if x==\"Cnon\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GChp_bI-zztb"
   },
   "source": [
    "### Extraction of the learning and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJxen2nLzztb",
    "outputId": "920ffeea-b088-4ffa-878f-fa29699d88d1"
   },
   "outputs": [],
   "source": [
    "rd_seed = 111 # Modify this sieve initialization\n",
    "npop = len(vispremv)\n",
    "#TODO Find xApp,xTest,yApp,yTest with the help of train_test_split\n",
    "#We will place 200 samples in the test set\n",
    "xApp,xTest, yApp, yTest = train_test_split( vispremR , y, test_size = 200, random_state = rd_seed )\n",
    "\n",
    "xApp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TguE-XvIrW_F",
    "outputId": "dad39560-fe45-408e-ce40-845a0c57d01c"
   },
   "outputs": [],
   "source": [
    "vispremv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cpamcf7Szztc"
   },
   "source": [
    "## [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "This \"old\" method is still commonly used, mainly because of its simplicity, interpretability and diffusion over years.\n",
    "\n",
    "### Estimation and optimization\n",
    "The model selection procedure involves a statistical penalization: *ridge*, Lasso or a combination of both (*elastic net*). The behaviour of the model selection in Python is not exactly comparable with the one used in R (*stepwise, backward, forward*) that optimizes *AIC*\n",
    "\n",
    "#### *Lasso* Optimization\n",
    "\n",
    "\n",
    "We first consider the **Lasso** penalty, which is proportional to the L1 norm of the  ou la pénalité est proportionnelle à la norme 1 de l'estimateur (ce qui parameter we are looking for. This penalty induces sparsity of the response coefficient vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WplPutAtsVS"
   },
   "source": [
    "We recall that the logistic regression model defined within our framework aims at estimating the conditional probability that a card-owner possesses the set of attributes $X$=$x$ (e.g. up to 10 months of consumer seniority, with at most 50 transfers per months on average, at most 2 life insurances etc..). The underlying parametric assumption states that, in our sample, the conditional odds of owning a card i.e., $ \\frac{P(Y=1|X)}{1-P(Y=1|X)}$ has a logit linear in $X$ (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "WUyJUf_YwrAZ",
    "outputId": "4a4fbb76-4a06-4bf0-88a0-09845adec88f"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#sns.regplot( x = vispremv['ITAVCL'], y = vispremv['CARVP'], data = vispremv, logistic = True )\n",
    "sns.set_palette(\"husl\", 3)\n",
    "sns.color_palette(\"husl\", 3)\n",
    "ax = sns.regplot( x = vispremv['MOYRVL'], y = vispremv['CARVP'], data = vispremv, logistic = True, color= 'pink' )\n",
    "sns.axes_style(\"whitegrid\")\n",
    "ax.set( xlabel = \"Moyenne des mouvements nets créditeurs des 3 mois en Kf\", ylabel = \"Possession de la carte Visa Premier\" )\n",
    "plt.title(\"Régression Logistique de la possession de carte Visa Premier sur la Moyenne des mouvement nets créditeurs\")\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "gB-GObSZ5FYS",
    "outputId": "0af3d50e-cc64-4314-faa4-defb8b349e35"
   },
   "outputs": [],
   "source": [
    "sns.barplot( x = vispremv['SEXEQ'],y = vispremv['CARVP'] , data = vispremv )\n",
    "vispremv.groupby( 'SEXEQ' ,as_index = False ).CARVP.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCaPZ8c4BKum"
   },
   "source": [
    "We observe a substantial over-representation of male card-owners in our sample (i.e., 45% versus 14% of for males and females respectively), it can potentially gives rationale for results of our models throughout the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 968
    },
    "id": "Iaygx_mY52AY",
    "outputId": "33237a1f-8ad1-449d-ad86-011961d0dd5e"
   },
   "outputs": [],
   "source": [
    "vispremv.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmpLMMOFDBTc",
    "outputId": "417e2cfe-1d19-454b-fa91-720f1681736c"
   },
   "outputs": [],
   "source": [
    "vispremv.corr().unstack().sort_values( ascending = False ).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "CoV9qqxgC0vy",
    "outputId": "8b902a59-ebf0-4bd1-cece-eb7cf6b3c6f8"
   },
   "outputs": [],
   "source": [
    "corr = vispremv.corr()\n",
    "kot = corr[abs( corr ) >= .5]\n",
    "plt.figure( figsize = ( 12,8 ) )\n",
    "sns.heatmap( kot, cmap = \"Blues\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1lNDA076F4F",
    "outputId": "bdf24cb9-346a-47e9-8e58-c5a62da31716"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model = sm.Logit( yApp ,xApp.loc[:,~xApp.columns.duplicated()] )\n",
    "result = logit_model.fit( maxiter = 500, method = 'nm' )\n",
    "print( result.summary2() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTdSP5b5BZ7Z"
   },
   "source": [
    "With the classic logistic regression method, the matrix of predictors $X$ is not singular because of high correlation (see below) and is thus not invertible. The gradient descent estimates are not consistent and we need to used to a penalyzed version of the logistic regression using Lasso or Ridge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "TJKU9owxAVv0",
    "outputId": "fedc3c1b-ec13-4155-f46f-41636a6c38a6"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.clustermap( xApp.loc[:,~xApp.columns.duplicated() ].corr() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxVTvznDFaa6"
   },
   "source": [
    "We hereby fit the data to a Lasso Logit Regression Model in order to reduce the number of regressors considered in the prediction of Visa credit card ownership.\n",
    "In particular, the objective loss function is equal to the negative log-likelihood function $L(X;\\beta)$ added to a regularization parameter $\\lambda$ controling for the $X$ matricial size $p$ with the $L_1$ norm (after convex relaxation of the optimization problem with the $L_0$ norm in order to have a sparse model). The minimization problem thus involves the estimation of the new parameter $\\lambda$ prior to the predictors parameters $\\beta$. The final problem is a convex optimization problem and requires Slater and KKT assumptions to be validated.\n",
    "\n",
    "$min_{\\lambda, \\beta} S(\\lambda;\\beta) = L(X;\\beta) + \\lambda|\\beta|_{1}$ $\\space \\space \\space \\space \\space \\space \\space \\space \\space (1.1)$\n",
    "\n",
    "where $ L(X;\\beta) = -log(\\prod_{i:Y_i = 1} p(X_i) \\prod_{j:Y_j = 0} (1-p(X_j)) $\n",
    "and $p(X)=\\frac{e^{X^T\\beta}}{1 + e^{X^T\\beta}}$ \n",
    "\n",
    "which is equivalent to minimizing the cross-entropy loss function.\n",
    "Thanks to the log-transformation of the likelihood function, we recover a convex optimization problem that can be solved using gradient descent methods.\n",
    "\n",
    "The Lasso thus shrinks the $\\beta$ coefficient estimates towards zero, with some of those coefficient estimates being exactly equal to zero when $\\lambda$ goes to infinity. Provided that $\\lambda$ is chosen appropriately, the Lasso estimator $\\hat\\beta(\\lambda)$ is consistent. The method yields a $sparse$ model that calls for a cross-validation of the $\\lambda$ parameter estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79184BI2P5hZ"
   },
   "source": [
    "The $k$-folded cross-validation procedure consists in defining a grid of possible values for the $\\lambda$ parameter to explore, partitioning the training sample in $k=5$ sub-samples and fitting the loss function in (1.1) for a fixed value of $\\lambda$ on all partitions but partition $k$. For each value of $\\lambda$ defined by the grid, we sum the five residuals obtained, one cell of the grid at a time. We then compute the residuals sum of squares on the $ Grid(\\lambda) $ and minimize such convex function with respect to $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJbn3MUkWca6"
   },
   "source": [
    "To conclude, the Lasso coefficient can be thought as a constraint optimization program's solution. Indeed, it exactly corresponds to the vector of $p$ estimates that lead to the smallest cross-entropy, subject to the constraint that there is a budget $s$ for how large the estimates can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQq_FBurzztc"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Grid of penalty parameters\n",
    "param = [ {\"C\": [ 0.5, 1, 5, 10, 12, 15, 30 ]} ] # dictionary of the values of C used for GridSearchCV\n",
    "# TODO\n",
    "# 1. Define a model (logistic regression)\n",
    "# 2. Use the penalty=\"l1\"\n",
    "# 3. Use GridSearchCV to obtain automatically the best regularization parameter \n",
    "# 4. In what follows, this method will be denoted by logitLasso !\n",
    "\n",
    "baseline_reg = LogisticRegression( penalty = 'l1',random_state = 0, solver = 'liblinear' ).fit( xApp, yApp )\n",
    "\n",
    "CV_reg =GridSearchCV(LogisticRegression( penalty = 'l1', solver = 'liblinear' ), param, cv = 5 )\n",
    "logitLasso = CV_reg.fit( xApp, yApp )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8lxGldIzztd",
    "outputId": "02e216e5-7aea-45b9-a2e0-d9a49ffdae30"
   },
   "outputs": [],
   "source": [
    "# Computation of the loss\n",
    "print(\"Best learning rate = %f, Best parameter  = %s\" %\n",
    "      ( 1.-logitLasso.best_score_,logitLasso.best_params_ ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best learning rate =\",\n",
    "      ( 1. - baseline_reg.score( xApp, yApp ) ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best learning rate among all logistic regression models trained on the xApp sub-sample yielded 23.9% of target yApp accurately predicted.\n",
    "The value of the parameter estimate associated with such learning rate (i.e. $\\lambda$) equaled 0.5. That is, when the size of the span of sparse $\\beta$ parameters increases by one, the loss in prediction approximated by the RSS mechanically increases by 0.5. \n",
    "\n",
    "We expect the prediction error to be lower than the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_vyqrMcRL6s"
   },
   "outputs": [],
   "source": [
    "CV_reg_2 =GridSearchCV(LogisticRegression( penalty = 'l1', solver = 'saga', max_iter = 5000  ), param, cv = 5 )\n",
    "logitLasso_2 = CV_reg_2.fit( xApp, yApp )\n",
    "# with saga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raLZPbEKRYty",
    "outputId": "e3f017ae-6960-45a9-8d89-85e51287ce5a"
   },
   "outputs": [],
   "source": [
    "print(\"Best learning rate = %f, Best parameter  = %s\" %\n",
    "      ( 1.-logitLasso_2.best_score_,logitLasso_2.best_params_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTpjmh6Hzzte"
   },
   "source": [
    "Prediction error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XA_eOwnxzzte",
    "outputId": "184a940b-39e5-44cd-e59e-578bd93acea9"
   },
   "outputs": [],
   "source": [
    "# Prediction with the model\n",
    "yChap = logitLasso.predict( xTest )\n",
    "# TODO:  compute the  confusion matrix with the help of pd.crosstab\n",
    "table = pd.crosstab( yChap, yTest )\n",
    "print( table )\n",
    "\n",
    "# Error on the test set\n",
    "print(\"Test error - Logistic regression  Lasso = %f\" % ( 1-logitLasso.score( xTest, yTest ) ) )\n",
    "# the false negative is the most costly error: we predict no card when the individual actually has one\n",
    "# loss of a client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the prediction error, i.e., the percentage of falsly predicted labels $\\hat y$ versus $yTest$ made on the first-seen observations $xTest$, equals 26% and not 23.9% as before on the training sub-samples.\n",
    "The highest prediction error type is the rate of false negative, when the algorithm predicts no card when the individual actually has one, the bank thus loses a client. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo_8DuT4zztf"
   },
   "source": [
    "#### *Ridge* Optimization \n",
    "On considère maintenant l'optimisation Ridge, ou la pénalité est proportionnelle à la norme 2 de l'estimateur (au carré). Ici, la solution optimale n'est pas parcimonieuse, il n'y a donc pas de sélection de variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTQgjc3Vzztf"
   },
   "outputs": [],
   "source": [
    "# Grid of penalty parameters\n",
    "param = [ {\"C\":[ 0.5,1,5,10,12,15,30 ]} ]\n",
    "# TODO\n",
    "# 1. Define a model (logistic regression)\n",
    "# 2. Use the penalty=\"l2\"\n",
    "# 3. Use GridSearchCV to obtain automatically the best regularization parameter \n",
    "# 4. In what follows, this method will be denoted by logitRidge!\n",
    "\n",
    "\n",
    "baseline_reg_ridge = LogisticRegression( penalty = 'l2',random_state = 0, solver = 'lbfgs', max_iter = 1000 ).fit( xApp, yApp )\n",
    "\n",
    "CV_reg_ridge = GridSearchCV( LogisticRegression( penalty = 'l2', solver='lbfgs' ), param, cv = 5 )\n",
    "logitRidge = CV_reg_ridge.fit( xApp, yApp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwoWpqcizztg",
    "outputId": "fa25034a-4a3a-4a06-aea5-20172c9ccb06"
   },
   "outputs": [],
   "source": [
    "# Error computation  \n",
    "print(\"Best score = %f, Best parameter = %s\" %\n",
    "      (1. - logitRidge.best_score_, logitRidge.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best learning rate =\",\n",
    "      ( 1. - baseline_reg_ridge.score( xApp, yApp ) ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RHd7XIuzztg",
    "outputId": "2d523160-74a3-4271-c501-629fc36e7b6b"
   },
   "outputs": [],
   "source": [
    "# Prediction \n",
    "yChap_ridge = logitRidge.predict(xTest)\n",
    "\n",
    "#Todo confusion matrix \n",
    "print(\"Erreur de test - Logistic regression  Lasso = %f\" % ( 1-logitLasso.score( xTest, yTest ) ) )\n",
    "\n",
    "\n",
    "# Error on the test set\n",
    "print(\"Erreur de test régression Ridge après validation croisée = %f\" % ( 1-logitRidge.score( xTest, yTest ) ) )\n",
    "\n",
    "table_ridge = pd.crosstab( yChap_ridge, yTest )\n",
    "print( table )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVBJK7Ln4UZt",
    "outputId": "488ddae8-9533-40f6-a81e-36516461d212"
   },
   "outputs": [],
   "source": [
    "print( \"Erreur de test régression Ridge avant validation croisée = %f\" % ( 1 - baseline_reg_ridge.score( xTest, yTest ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSHVVRtH44nQ",
    "outputId": "55d09cd4-d997-47cf-bc57-bf50f0a138e1"
   },
   "outputs": [],
   "source": [
    "# Prediction with the Ridge model\n",
    "yChap_ridge = baseline_reg_ridge.predict( xTest )\n",
    "# TODO:  compute the  confusion matrix with the help of pd.crosstab\n",
    "table_ridge = pd.crosstab( yChap_ridge, yTest )\n",
    "print( table_ridge )\n",
    "\n",
    "# Error on the test set\n",
    "print(\"Test error - Logistic regression  Ridge before cross validation = %f\" % ( 1-baseline_reg_ridge.score( xTest, yTest ) ) )\n",
    "# the false negative is the most costly error: we predict no card when the individual actually has one\n",
    "# loss of a client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOBXUxPZ5UI8",
    "outputId": "d2d00563-aec5-4453-91fd-03ceae2f8426"
   },
   "outputs": [],
   "source": [
    "baseline_reg_ridge.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOIWBxrA5YUq",
    "outputId": "461e9db4-6337-49c1-bbd8-d2d194014c4f"
   },
   "outputs": [],
   "source": [
    "logitRidge.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XV-E5haE-PPU",
    "outputId": "dfe6ba2c-70b2-41b8-ab15-3d37acc96a48"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "abs(r2_score(y_pred  = logitRidge.predict(xTest), y_true = yTest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rGWzM9Tzztg"
   },
   "source": [
    "**Q** Note the prediction errors and compare them with the ones predicted by the cross validation step.\n",
    "The prediction errors are the same after cross-validation for Ridge and Lasso (the cardinal of the non-sparse span of parameter is rather small, 7). \n",
    "On the one hand, the learning error is slightly higher before than after cross-validation (when comparing with the prediction error of the best cross-validated model, $\\lambda$ = 0.5) with Lasso and Ridge: 0.23986095017381226 ~ 0.24 vs 0.239844 ~ 0.24 .\n",
    "On the other hand, the prediction error is exactly the same before and after cross-validation: 0.26, with both methods.   \n",
    "One can attribute those equalities to the fact that the size of parameters $p$ is lower than $n$, and so that the problem at hands is more one of high correlation than of high dimensionality. Lasso and Ridge are thus statistically equivalent. A Logit might be the best-suited model to our data. \n",
    "### Interpretation\n",
    "\n",
    "LassoOpt produced by GridSearchCV does not record the values of the parameters learnt by the model. It is then necessary to launch another time this model with the optimal value of the parameter if we wish to show the values of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZfbh3Gpzzth",
    "outputId": "689b7f4e-fd91-4413-be0c-9b834814a93d"
   },
   "outputs": [],
   "source": [
    "LassoOpt = LogisticRegression(penalty = \"l1\",C = 12, solver = 'liblinear' )\n",
    "LassoOpt = LassoOpt.fit( xApp, yApp )\n",
    "# Storage of the coefficients\n",
    "vect_coef = np.matrix.transpose( LassoOpt.coef_ )\n",
    "vect_coef = vect_coef.ravel()\n",
    "#Show the most important 25 coefficients\n",
    "coef = pd.Series( abs( vect_coef ),index = xApp.columns ).sort_values( ascending = False ) \n",
    "print( coef )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the parameter estimate associated to the variable PSPQ_Pint is set to 0: sparcity of the Lasso model. $\\hat\\beta_{FAMIQ_Fcou}$ is also very close to 0 but not exactly 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "vMTUYzrOzzth",
    "outputId": "2c41d45d-4c7b-4ff9-c0c0-b56a8e0a0158"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 7,4 ) )\n",
    "coef.plot( kind = 'bar' )\n",
    "plt.title( 'Coeffients' ) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plmejigrzzth"
   },
   "source": [
    "**Q** What are the important variables? How to draw some conclusion/interpretation?\n",
    "The most important variables are the socio-professional categories (i.e. executive), and the sex of the individual (i.e. male). \n",
    "Being a male executive thus seems to be highly correlated with card ownership. It is also posible that this first result reflects an overpresentation of male in executive branches. Since executive branches are associated with financial wealth and extensive asset management, it only makes sense to see the wealthiest individuals of the sample as more likely to own credit card. \n",
    "\n",
    "**Q** Is the Lasso penalty efficient?\n",
    "\n",
    "The lasso penalty is equivalent to Ridge and elastic net penalties. Lasso reduces the model complexity by yielding sparse solutions to the optimization problem, at the cost of an additional biais in the $\\beta$ parameters estimation  (variance-biais trade-off).\n",
    "The results obtained with Ridge and *elastic net* are the same as the ones computed with Lasso so that the efficiency properties are comparable in our special case. \n",
    "\n",
    "We propose a slightly different of lasso, called adaptive lasso in order to reduce the biais of the estimations (Fan and Li (2001) for the oracle framework, Zou (2006) for the adaptive lasso).\n",
    "\n",
    "The objective of adapative lasso is to weight the $L_1$ norm of the $\\beta$ associated to the regularization term $\\lambda$ in order to correct the bias: \n",
    "\n",
    "$min_{\\lambda, \\beta} S(\\lambda;\\beta) = L(X;\\beta) + \\lambda w|\\beta|_{1}$ $\\space \\space \\space \\space \\space \\space \\space \\space \\space (1.1)$\n",
    "\n",
    "with $w$ a vector of weight parameters, and $L(X;\\beta)$ $ = \\frac{1}{2}{||Y-X\\beta||^2}_2$\n",
    "\n",
    "\n",
    "The weights estimates $\\hat w_j$ are simply equal to $\\frac{1}{|\\hat\\beta_j|} \\forall j \\in [1;p] $\n",
    "\n",
    "*Elastic net is a mixture of ridge and lasso, and gives more balanced results in high dimension. With $p$ = 7, it is equivalent to lasso and ridge. \n",
    "\n",
    "It would be interesting to compare with the *ridge* and *elastic net* model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = 0.01\n",
    "g = lambda w: np.log(ll + np.abs(w))\n",
    "gprime = lambda w: 1. / (ll + np.abs(w))\n",
    "p_obj = lambda w: 1. / (2 * n_samples) * np.sum((yApp - np.dot(xApp, w)) ** 2) \\\n",
    "                  + alpha * np.sum(g(w))\n",
    "\n",
    "n_samples, n_features = xApp.shape\n",
    "weights = np.ones(n_features) #gets the number of weights that needs to be computed for Adaptive Lasso\n",
    "n_lasso_iterations = 5 # \n",
    "\n",
    "for k in range( n_lasso_iterations ):\n",
    "    X_w = xApp / weights[np.newaxis, :]\n",
    "    alasso_reg = LogisticRegression( penalty = 'l1',random_state = 0, solver = 'liblinear' )\n",
    "    alasso_reg.fit( X_w, yApp )\n",
    "\n",
    "    CV_reg_alasso =GridSearchCV(LogisticRegression( penalty = 'l1', solver = 'liblinear' ), param, cv = 5 )\n",
    "    logitALasso = CV_reg_alasso.fit( X_w, yApp )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of the loss\n",
    "print(\"Best learning rate = %f, Best parameter  = %s\" %\n",
    "      ( 1.-logitALasso.best_score_,logitALasso.best_params_ ) )\n",
    "# same learning rate as with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with the model\n",
    "for k in range( n_lasso_iterations ):\n",
    "    X_w_test = xTest / weights[np.newaxis, :]\n",
    "    yChap_ALasso = logitALasso.predict( X_w_test )\n",
    "# TODO:  compute the  confusion matrix with the help of pd.crosstab\n",
    "table = pd.crosstab( yChap_ALasso, yTest )\n",
    "print( table )\n",
    "\n",
    "# Error on the test set\n",
    "print(\"Test error - Adaptive Lasso Logistic regression  Lasso = %f\" % ( 1-logitALasso.score( X_w_test, yTest ) ) )\n",
    "# the false negative is the most costly error: we predict no card when the individual actually has one\n",
    "# loss of a client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmHkkqIXTChP",
    "outputId": "79c9c869-6212-447d-f076-c9a444052eb9"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "Logit_elastic_net = LogisticRegressionCV( cv = 5, penalty = 'elasticnet', l1_ratios = [0.1, 0.5, 0.9], solver = 'saga', max_iter = 5000  )\n",
    "Logit_elastic_net.fit( xApp, yApp )\n",
    "\n",
    "print(\"Logistic Regression: {} || Elasticnet: {}\".format(logitLasso.score( xApp, yApp ), Logit_elastic_net.score( xApp, yApp ) ) )\n",
    "\n",
    "# Print out some more metrics\n",
    "print( \"Logistic Regression\" )\n",
    "print( classification_report( yApp , logitLasso.predict( xApp ) ) )\n",
    "print( \"Elastic Net\" )\n",
    "print( classification_report( yApp, Logit_elastic_net.predict( xApp ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNJ0zmaiU-M0",
    "outputId": "7c0062dd-4bb6-418d-e755-ab1c6238bea2"
   },
   "outputs": [],
   "source": [
    "print(\"Logistic Regression: {} || Elasticnet: {}\".format(logitRidge.score( xApp, yApp ), Logit_elastic_net.score( xApp, yApp ) ) )\n",
    "\n",
    "# Print out some more metrics\n",
    "print( \"Logistic Regression\" )\n",
    "print( classification_report( yApp , logitRidge.predict( xApp ) ) )\n",
    "print( \"Elastic Net\" )\n",
    "print( classification_report( yApp, Logit_elastic_net.predict( xApp ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "fhe8trCDzzth",
    "outputId": "085ca605-43e1-4b12-9c4a-90590d2b7cf2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "listMethod = [ [\"Lasso\",logitLasso],[\"Ridge\",logitRidge] ]\n",
    "\n",
    "for method in enumerate( listMethod ):\n",
    "    probas_ = method[1][1].predict_proba( xTest )\n",
    "    fpr, tpr, thresholds = roc_curve( yTest, probas_[:,1] )\n",
    "    plt.plot( fpr, tpr, lw=1,label = \"%s\"%method[1][0] )\n",
    "plt.xlabel( 'False positive rate' )\n",
    "plt.ylabel( 'True positive rate' )\n",
    "plt.legend( loc = \"best\" )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CHHUxyCjuO3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "listMethod = [ [\"Lasso\",logitLasso],[\"Elastic net\",Logit_elastic_net] ]\n",
    "\n",
    "for method in enumerate( listMethod ):\n",
    "    probas_ = method[1][1].predict_proba( xTest )\n",
    "    fpr, tpr, thresholds = roc_curve( yTest, probas_[:,1] )\n",
    "    plt.plot( fpr, tpr, lw = 1,label = \"%s\"%method[1][0] )\n",
    "plt.xlabel( 'False positive rate' )\n",
    "plt.ylabel( 'True positive rate' )\n",
    "plt.legend( loc=\"best\" )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPjOqix-kGiX"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "listMethod = [ [\"Ridge\",logitRidge],[\"Elastic net\",Logit_elastic_net] ]\n",
    "\n",
    "for method in enumerate( listMethod ):\n",
    "    probas_ = method[1][1].predict_proba( xTest )\n",
    "    fpr, tpr, thresholds = roc_curve( yTest, probas_[:,1] )\n",
    "    plt.plot( fpr, tpr, lw = 1,label = \"%s\"%method[1][0] )\n",
    "plt.xlabel( 'False positive rate' )\n",
    "plt.ylabel( 'True positive rate' )\n",
    "plt.legend( loc=\"best\" )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf7KpbQnzzti"
   },
   "source": [
    "## Discriminant analysis\n",
    "Three methods are availables: parametric ones with LDA-QDA and a non parametric one (*k* nearest neighbor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0yl_Rctzzti"
   },
   "outputs": [],
   "source": [
    "from sklearn import discriminant_analysis\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lznnJwj9zzti"
   },
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "Estimation of the model (there is no feature selection step) and then prediction over the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHQhb8J-QsGQ"
   },
   "source": [
    "\n",
    "<p style='text-align: justify;'\n",
    "\n",
    "Linear Discriminant Analysis was first proposed by Fisher in 1936 as a method to predict (well separated) $k$ classes, in response to logistic regression model's instabilities. We hereby study the case of a two-category classification problem i.e., card-ownerniship or no. Using Bayes' Theorem to derive the best classifier, i.e., with the lowest possible $total$ error rate out of all classifiers (provided that the Gaussian model is correct): \n",
    "\n",
    "$P(Y=1| X=x)$ $= \\frac{{\\pi_k}{f_k}(x)}{\\sum \\limits_{l=1}^{K}{{\\pi_l}{f_l}}(x) } $ $ \\forall k = $  {$0;1$}      $\\space$ $\\space$ $\\space$ $\\space$ $\\space$  $\\space$ $\\space$ ($1.1$)\n",
    "\n",
    "\n",
    "such that \n",
    "\n",
    "${\\pi_k}$ being the prior probability that a random chosen observation belongs to the class of card-owners coded as $k=1$ \n",
    "\n",
    "${f_k}(X)= P(X=x| Y=k)$  being the conditional density function of $X$ for an observation belonging to the $k^{th}$ class $ \\forall k = $  {$0;1$}\n",
    "\n",
    "\n",
    "We thus estimate prior ${\\pi_k}$ and the conditional probability density function ${{f_k}}(X)$ by $\\hat{\\pi_k}$ and  $\\hat{{f_k}}(X)$ respectively in order to plug them in the posterior $(1.1)$. \n",
    "\n",
    "We assume that the predictors $X$ are normally distributed according to a $N(\\mu_k, \\Sigma)$ with a class-specific mean-vector of size ($k$x$1$) and homoskedastic, that is, there is a common covariance matrix $\\Sigma$ of size ($p$x$p$) across both classes. We plug the resulting multivariate gaussian distribution function in $(1)$ as the $\\hat{{f_k}}(X)$.\n",
    "The unbiased maximum likelihood estimates  $\\hat\\mu_k$ and $\\hat\\sigma^2$ for $\\mu_k$ and $\\sigma^2$ are used to compute ${{f_k}}(X)$ $ \\forall k, \\space \\space  \\forall p  $: \n",
    "\n",
    "\n",
    "$\\hat\\mu_{k,p} = \\frac{1}{n_k}\\sum \\limits_{i:y_i =k}{}x_{i,p}$ \n",
    "\n",
    "$\\hat\\Sigma = \\frac{1}{n_K}(x_i - \\hat\\mu_k)^T(x_i - \\hat\\mu_k)\n",
    "  \\forall k = $  {$0;1$} $ \\space \\space \\space \\space \\space  \\space \\space (1.2)$\n",
    "\n",
    "\n",
    "Naturally, $\\hat{\\pi_k}$ is equal to the empirical sample share of individual belonging to class $k$:\n",
    "\n",
    "\n",
    "$\\hat{\\pi_k}$ = $\\frac{n_k}{n} \\forall k = $  {$0;1$} $\\space$ $\\space$ $\\space$ $\\space$ $\\space$  $\\space$ $\\space$ ($1.3$)\n",
    "\n",
    "\n",
    "The Bayes classifier thus assigns an observation $X=x$ to the class for which the estimate $\\hat{P}(Y=1| X=x)$ is largest (i.e. greater than $1/2$).\n",
    "Taking the log of ($1.1$) with $\\hat{{f_k}}(X)$ and $\\hat{\\pi_k}$ and simplifying the constant terms yields the following Bayes decision boundary rule: \n",
    "\n",
    "\"$Assign$ $y_i$ $to$ $k=1$ $\\iff$ $2x^T(\\mu_1 - \\mu_2) + log(\\pi_1) - log(\\pi_2) \\ge {\\mu_1}^T{\\mu_1} - {\\mu_2}^T{\\mu_2}$ and to $k=0$ otherwise.\"  ($1.4$)\n",
    "\n",
    "\n",
    "The Bayes decision boundary thus corresponds to the point where \n",
    "$ x= 2(\\mu_1 - \\mu_2)^{-1}({\\mu_1}^T{\\mu_1} - {\\mu_2}^T{\\mu_2} -log(\\frac{\\pi_1}{\\pi_2})) = \\frac{1}{2}[({\\mu_1 + \\mu_2})-(\\mu_1 - \\mu_2)^{-1}({log(\\frac{\\pi_1}{\\pi_2})})]$ $\\space$ $\\space$ $\\space$ $\\space$ $\\space$  $\\space$ $\\space$ ($1.5$)\n",
    "\n",
    "The $LDA$ classifier plugs the estimates given in $(1.2)$ and $(1.3)$ into ($1.4$) in order to assign an observation $X=x$ to the most probable class.\n",
    "\n",
    "The performance metric of interest here is the test error rate (not be mistaken with the training error rate). We expect the classifier to perform worse if we were to predict a new set of individuals would own credit cards or not. Indeed, the fitting of the model i.e. the parameters estimation is specifically done on the training set. Nonetheles, the train test split has been randomized and the ratio of parameters $p$ to number of samples $n$ as $\\rho= \\frac{14}{1063}=0.013$ is close to 0 so we do not expect overfitting. \n",
    "\n",
    "To that extent, a bank or a credit card company might be particulary sensitive to false negatives, that is, individuals who $are$ card owners but who are misclassified as $non-card$ $owners$ by the $LDA$ classifiers.\n",
    "\n",
    "\n",
    "It might be that the initial threshold of $50%$ for the posterior probability in the two-class case fails short at meeting such needs. We could instead lower that threshold to say 20% in order to decrease the false negative rate, at the expense of an increase in the false positive rate that is less costly to a bank trying to grow its consumer base\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kE5xXsIWCdis",
    "outputId": "c32cf5ee-affd-4a60-9bb0-a831f31f5ccc"
   },
   "outputs": [],
   "source": [
    "yApp.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANc-r6LGELpA"
   },
   "outputs": [],
   "source": [
    "def prior(df):\n",
    "  ratio = df[df==1].size/df.size\n",
    "  return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-lrVgPrEr_O",
    "outputId": "82855b7c-7bf8-49f1-eb16-ccacd845c3c8"
   },
   "outputs": [],
   "source": [
    "prior(yApp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmcjK1fzzztj",
    "outputId": "728bf7fd-4c4d-4ad0-af9f-e7c787fae6d8"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Define the model ('lda' as discriminant_analysis.LinearDiscriminantAnalysis)\n",
    "# 2. Fit the model\n",
    "# 3. Predict on the test set\n",
    "# 4. Compute the confusion matrix\n",
    " \n",
    "lda = discriminant_analysis.LinearDiscriminantAnalysis( solver = 'svd', shrinkage = None, priors = None, n_components = None, store_covariance = False, tol = 0.0001 )\n",
    "lda.fit(xApp, yApp)\n",
    "# Prediction on the test set\n",
    "print(\"Erreur de test lda = %f\" % (1-lda.score(xTest,yTest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJds2Kd4KW9P",
    "outputId": "67aa1dd3-ae83-47ee-fa0a-da4ec8f3b377"
   },
   "outputs": [],
   "source": [
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "D3suK-2HEGPo",
    "outputId": "4987e7ee-5f67-42c7-9465-ae92892c78d4"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame( confusion_matrix( yTest, lda.predict( xTest ) ) )\n",
    "# same results as with Ridge, Lasso and Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "1A8oZfO4EmMB",
    "outputId": "3a600b72-8cf5-4a5f-a59a-755749caf2f4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix( lda, xTest, yTest, display_labels = yTest.values, cmap = plt.cm.Blues )  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9xb4yREzztk"
   },
   "source": [
    "**Q** What about the quality of the prediction? The ability of interpret the method?\n",
    "\n",
    "The quality of the prediction is exactly the same as with Lasso, Ridge and Elastic net in terms of metrics. Still, the interpretability is rather trickier than previously since the method involves Bayesian probability theory and \"reversed learning\" so-to-speak.\n",
    "\n",
    "**Q** What is the meaning of the  *warning*? What are the variables involved by this warning?\n",
    "The dummy variables are colinear. \n",
    "As we saw previously throughout the Linear Discriminant Analysis, the decision rule maximizes the posterior probability that a sample belongs to a group \"card_owner\" or \"no card_owner\" and can be written as  \n",
    "\n",
    "$L_i = (X - \\mu_i)^t \\Sigma_i (X - \\mu_i) + log(|\\Sigma_i| - 2log(\\pi_i) \\forall i \\in [0;1]$\n",
    "\n",
    "<--> with estimates plugged-in\n",
    "\n",
    "$\\hat L_i = (X - \\hat\\mu_i)^t \\hat\\Sigma_i (X - \\hat\\mu_i) + log(|\\hat\\Sigma_i| - 2log(\\pi_i) \\forall i \\in [0;1]$\n",
    "\n",
    "\n",
    "In fact, if we reformlate $\\hat L_i $ in Spectral terms with a little bit of Linear Algebra, we can rewrite the matrices \n",
    "$(X - \\mu_i)^t (X - \\mu_i)$ is a squared matrix, positive definite, and can be diagonalized if the $X$ variables *are not* colinear. \n",
    "The equation can be expresses as a polynome in $X$ of order 2. It is thus a linear application of $X$ and when the matrix is invertible, $(A - \\lambda I_p)X = 0 $ has a unique solution.\n",
    "\n",
    "One can thus re-write $\\hat L_i$ as a linear combination of the covariance matrix eigenvalues and eigenvectors, minus a contribution from the prior probabilities.\n",
    "When the $X$ are highly colinear, the eigenvalues are very close to 0, and their inverse are thus very large, $\\hat L_i$ will in turn be very large and its variance will tends to infinity. This is a first instability issue. \n",
    "\n",
    "The second issue related to colinearity in the context of Quadratic Discriminant Analysis is the closeness between the directions spanning  $\\hat L_i$. Because $\\hat L_i$ is a linear combination of very similar vectors, it is ill-suited to distinguish between the groups (the information provided by the data are too similar!). One could only consider the $K$ largest eigenvalues instead to run QDA when there is a colinearity issue (Noes and al., 2001). Alternatively, one could look at the VIF or at correlations and select variables according to an arbitrary threshold. \n",
    "\n",
    "\n",
    "\n",
    "### Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zW-C5zF2zztk",
    "outputId": "02c17736-9952-48f9-d0a9-032348455820"
   },
   "outputs": [],
   "source": [
    "# Same procedure as the one of LDA for QDA\n",
    "\n",
    "# TODO:\n",
    "# 1. Define the model ('qda' as discriminant_analysis.LinearDiscriminantAnalysis)\n",
    "# 2. Fit the model\n",
    "# 3. Predict on the test set\n",
    "# 4. Compute the confusion matrix\n",
    " \n",
    "qda = discriminant_analysis.QuadraticDiscriminantAnalysis( tol = 0.0001 )\n",
    "qda.fit( xApp, yApp )\n",
    "# Prediction on the test set\n",
    "print( \"Erreur de test lda = %f\" % ( 1-qda.score(xTest,yTest ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yChapQDA = qda.predict( xTest )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "QRSRh-caax6z",
    "outputId": "75a3d689-bf74-4472-e352-8b5d1f501f58"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix( qda, xTest, yTest, display_labels = xApp.columns, cmap = plt.cm.Blues )  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esh6ztARzztk"
   },
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrt8aIBhzztk"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Define the model with the 10 nearest neighbors (use KNeighborsClassifier)\n",
    "# 2. Fit the model\n",
    "# 3. Predict on the test set\n",
    "# 4. Show the confusion matrix \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "disKnn = KNeighborsClassifier( n_neighbors = 10 )\n",
    "disKnn.fit( xApp,np.ravel( yApp.values ) )\n",
    "\n",
    "print(table)\n",
    "# Prediction error on the test set\n",
    "print(\"Erreur de test knn = %f\" % (1-disKnn.score(xTest,yTest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(labels,pred):\n",
    "    c=0 #Initialise le compteur à 0\n",
    "    lim = int(len(pred))\n",
    "    for i in range(lim):\n",
    "        if pred[i]!=labels[i]:\n",
    "            c=c+1 #+1 au compteur si si la prediction est différente du labels (pour chaque valeur)\n",
    "    score=100-float(c)/lim*100 #Calcul du score\n",
    "    return score\n",
    "\n",
    "    prediction(labels_lr,pred_lr)\n",
    "\n",
    "pred_knn = disKnn.predict( xTest ) \n",
    "labels_knn = np.ravel( yTest.values )\n",
    "prediction( labels_knn,pred_knn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_knn = disKnn.score( xTest,np.ravel( yTest.values ) )\n",
    "score_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzB3aBs3zztl"
   },
   "outputs": [],
   "source": [
    "#Optimization of the smoothing parameter (number of neighbors) k\n",
    "#Grid \n",
    "param_grid = [{\"n_neighbors\":list( range( 1,15 ) ) }]\n",
    "disKnn = GridSearchCV( KNeighborsClassifier(),param_grid,cv=5,n_jobs=-1 )\n",
    "disKnnOpt = disKnn.fit( xApp, yApp ) # GridSearchCV is itself an estimator \n",
    "# Optimal parameter \n",
    "disKnnOpt.best_params_[\"n_neighbors\"]\n",
    "print(\"Best score = %f, Best parameter = %s\" % ( 1.-disKnnOpt.best_score_,disKnnOpt.best_params_) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDRONWXpzztl"
   },
   "outputs": [],
   "source": [
    "# Prediction on the test set \n",
    "yChap = disKnnOpt.predict( xTest )\n",
    "# Confusion matrix \n",
    "table=pd.crosstab( yChap,yTest )\n",
    "print( table )\n",
    "\n",
    "# Estimation of the prediction error on the test set \n",
    "print( \"Error rate of knn_opt = %f\" % ( 1-disKnnOpt.score( xTest,yTest ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg1mHDKczztm"
   },
   "source": [
    "ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xk2C2-Sbzztm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "# List of the methods \n",
    "listMethod=[[\"lda\",yChap],[\"qda\",yChapQDA],[\"knn\",disKnnOpt]]\n",
    "# Curves computation\n",
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].predict_proba(xTest)\n",
    "    fpr, tpr, thresholds = roc_curve(yTest, probas_[:,1])\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rUl53Pqzztm"
   },
   "source": [
    "## [Binary decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "Binary trees are competing well versu logistic regression and are commonly used in datascience. In particular, their interpretation are simple, which is a great advantage of this method. However, the optimization of the parameters involved in this method is somewhat versatile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lcv6a0rZzztm"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntKX4A6Szztn"
   },
   "outputs": [],
   "source": [
    "# Define and fit the model\n",
    "\n",
    "tr = tree.DecisionTreeClassifier()\n",
    "parameters = {\n",
    "    \"max_depth\":range(1,10),\n",
    "    \"min_samples_leaf\":range(1,10)}\n",
    "tr_opti = GridSearchCV(tr, parameters, cv=5, scoring='accuracy')\n",
    "tr_opti.fit(xApp, np.ravel(yApp.values))\n",
    "\n",
    "score_opti_tr=tr_opti.best_score_\n",
    "pred_opti_tr=tr_opti.predict(xTest)\n",
    "\n",
    "tr.fit(xApp,np.ravel(yApp.values))\n",
    "score_tr=tr.score(xTest,np.ravel(yTest.values))\n",
    "print(\"On observe que le meilleur score pour la méthode arbre de la décision est :\", score_opti_tr)\n",
    "print(\"Paramètres optimisés pour la méthode arbre de la décision :\",tr_opti.best_params_)\n",
    "print('l ancien score était', score_tr)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = tr.predict(xTest)\n",
    "labels_tr=np.ravel(yTest.values)\n",
    "prediction(labels_tr,pred_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyRkvqSzztn"
   },
   "source": [
    "**Q** What is the default homogeneity criterion used by this method?\n",
    "\n",
    "**Q** What is the major drawback of the pruning step in  `Scikkit-learn` when compared to the  `rpart` library in R?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDx159w-zzt0"
   },
   "outputs": [],
   "source": [
    "# Optimization of the depth of the tree\n",
    "#TODO\n",
    "\n",
    "# optimal parameter\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (1. - tr_opti.best_score_,tr_opti.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldrincNzzzt1"
   },
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "yChap = tr_opti.predict(xTest)\n",
    "# Confusion matrix\n",
    "table=pd.crosstab(yChap,yTest)\n",
    "print(table)# Prediction error on the test set\n",
    "print(\" Prediction error on the test set = %f\" % (1-tr_opti.score(xTest,yTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXZLzvQJzzt2"
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "!pip install --upgrade scikit-learn==0.20.3\n",
    "!pip install pydotplus\n",
    "from six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pydotplus\n",
    "treeG=DecisionTreeClassifier(max_depth=tr_opti.best_params_['max_depth'])\n",
    "treeG.fit(xApp,yApp)\n",
    "dot_data = StringIO() \n",
    "export_graphviz(treeG, out_file=dot_data) \n",
    "graph=pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_png(\"treeOpt.png\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXLc7AHEzzt2"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='treeOpt.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3yDHBt7zzt2"
   },
   "source": [
    "### [Roc curve ROC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "Comparison of the previous methods\n",
    "\n",
    "The default threshold   (0.5) is not necessarily the best one, and it is necessary to compare the ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7vBaFrVzzt2"
   },
   "outputs": [],
   "source": [
    "# Liste of the  methods \n",
    "listMethod=[[\"Logit\",logitLasso],[\"lda\",yChap],[\"Arbre\",tr_opti]]\n",
    "# Roc curves \n",
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].predict_proba(xTest)\n",
    "    fpr, tpr, thresholds = roc_curve(yTest, probas_[:,1])\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ww3nsy5zzt3"
   },
   "source": [
    "Comment the results.\n",
    "\n",
    "**Q** Interest of the logistic regression when compared to the LDA?\n",
    "\n",
    "**Q** Consequence of the ROC curve crossing on the AUC evaluation?\n",
    "\n",
    "The size of the test set (200) is modest..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9PtxZDZzzt3"
   },
   "source": [
    "## [Aggregation methods](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "We mainly compare here the three algorthms: *bagging, random forest, boosting*.\n",
    "\n",
    "### *Bagging*\n",
    "\n",
    "**Q** What is the default aggregated algorithm? \n",
    "\n",
    "**Q** What is the default number of estimators ? Is it necessary to optimize this number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbJ2YHmUzzt3"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag= base_estimator(SVC(), n_estimators=100, random_state=0) # Def model (nestim =100)\n",
    "bagC=bag.fit(xApp, yApp)\n",
    "# Prediction on the test set\n",
    "yChap = bagC.predict(xTest)\n",
    "# Confusion matrix\n",
    "table=pd.crosstab(yChap,yTest)\n",
    "print(table)\n",
    "\n",
    "# Prediction error on the test set\n",
    "print(\"Prediction error with bagging = %f\" % (1-bagC.score(xTest,yTest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRv9qorYzzt4"
   },
   "source": [
    "**Q** Run this previous cell several times. What do you think about the stability of the method and about the its reliability?\n",
    "\n",
    "### *Random forest*\n",
    "\n",
    "**Q** What is the parameter to be optimized for this algorithm? What is its default value?\n",
    "\n",
    "**Q** Is the number of trees a versatile parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui8diWdxzzt4"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtjrU0tozzt4"
   },
   "outputs": [],
   "source": [
    "# Optimization  of max_features\n",
    "param=[{\"max_features\":list(range(2,10,1))}]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100,max_depth=20,random_state=0) \n",
    "rf.fit(X_train,np.ravel(Y_train.values))  \n",
    "\n",
    "pred_rf = rf.predict(xTest)\n",
    "labels_rf=np.ravel(yTest.values)\n",
    "prediction(labels_rf,pred_rf)\n",
    "\n",
    "\n",
    "score_rf=rf.score(xTest,np.ravel(yTest.values))\n",
    "score_rf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rf = RandomForestClassifier(n_estimators=100,random_state = 0) \n",
    "parameters = {\n",
    "    \"max_depth\":range(1,15),\n",
    "    \"min_samples_leaf\":range(1,20)}\n",
    "rf_opti = GridSearchCV(rf,parameters,cv=5,scoring='accuracy')\n",
    "rf_opti.fit(xApp.values,np.ravel(yApp.values))\n",
    "\n",
    "score_opti_rf=rf_opti.best_score_\n",
    "pred_opti_rf=rf_opti.predict(xTest)\n",
    "\n",
    "print(\"On observe que le meilleur score pour la méthode Random Forest est :\", score_opti_rf)\n",
    "print(\"Paramètres optimisés pour la méthode Random Forest est :\",rf_opti.best_params_)\n",
    "print('l ancien score était', score_rf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rfOpt=rf.fit(xApp, yApp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# optimal parameter \n",
    "print(\"Best score = %f, Best parameter = %s\" % (1. - rfOpt.best_score_,rfOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4Zz1Ffszzt4"
   },
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "yChap = rfOpt.predict(xTest)\n",
    "# Confusion matrix\n",
    "table=pd.crosstab(yChap,yTest)\n",
    "print(table)\n",
    "\n",
    "# Prediction error on the test set\n",
    "print(\"Prediction error with  random forest opt -quantitative = %f\" % (1-rf_opti.score(xTest,yTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnLJfrlczzt4"
   },
   "source": [
    "### *Gradient boosting*\n",
    "\n",
    "**Q** What is the historical *boosting* algorithm? Which one is used now?\n",
    "\n",
    "**Q** What are the important parameters to be tuned? How to calibrate them??\n",
    "\n",
    "**Q** What is the default value of the parameter that is not optimized below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSKMtz23zzt5"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Optimization of two parameters\n",
    "paramGrid = [\n",
    "  {'n_estimators': list(range(100,601,50)), 'learning_rate': [0.1,0.2,0.3,0.4]}\n",
    " ]\n",
    "gbmC= GridSearchCV(GradientBoostingClassifier(),paramGrid,cv=5,n_jobs=-1)\n",
    "gbmOpt=gbmC.fit(xApp, yApp)\n",
    "# Optimal parameters\n",
    "print(\"Best score = %f, Best parameters = %s\" % (1. - gbmOpt.best_score_,gbmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSk8XVkCzzt5"
   },
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "yChap = gbmOpt.predict(xTest)\n",
    "# Confusion matrix\n",
    "table=pd.crosstab(yChap,yTest)\n",
    "print(table)\n",
    "\n",
    "# Prediction error on the test set\n",
    "print(\"Test error of gbm opt = %f\" % (1-gbmOpt.score(xTest,yTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXT7rtALzzt5"
   },
   "source": [
    "### Courbes ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y9fQ5R_zzt5"
   },
   "outputs": [],
   "source": [
    "# List of the methods\n",
    "listMethod=[[\"Logit\",logitLasso],[\"lda\",disLin],[\"Arbre\",treeOpt],[\"RF\",rfOpt],[\"GBM\",gbmOpt]]\n",
    "# ROC curves computation \n",
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].predict_proba(xTest)\n",
    "    fpr, tpr, thresholds = roc_curve(yTest, probas_[:,1])\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate ')\n",
    "plt.legend(loc=\"best\")\n",
    "send(plt,17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5Tsoaj_zzt6"
   },
   "source": [
    "**Q** What is the best interpretable method? What is the best method?\n",
    "\n",
    "**Q** What can you say about the *extrem gradient boosting* ? Number of parameters to be tuned? In Python? In R? Its diffusion?\n",
    "\n",
    "**Exercice** Add the deep learning and SVM family of methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "XGB = XGBClassifier()\n",
    "XGB.fit(X_train,np.ravel(Y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_XGB = XGB.predict(xTest)\n",
    "labels_XGB=np.ravel(yTest.values)\n",
    "prediction(labels_XGB,pred_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_XGB=XGB.score(xTest,np.ravel(yTest.values))\n",
    "score_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = XGBClassifier()\n",
    "parameters = {\n",
    "    \"max_depth\":range(3,10)}\n",
    "XGB_opti_2 = GridSearchCV(XGB,parameters,cv=5, scoring='accuracy')\n",
    "XGB_opti_2.fit(xApp,np.ravel(yApp.values))\n",
    "\n",
    "score_opti_XGB_2=XGB_opti_2.best_score_\n",
    "pred_opti_XGB_2=XGB_opti_2.predict(xTest)\n",
    "\n",
    "\n",
    "print(\"On observe que le meilleur score pour la méthode XGBoost est :\", score_opti_XGB_2)\n",
    "print(\"Paramètres optimisés pour la méthode XGBoost est :\",XGB_opti_2.best_params_)\n",
    "print(\"L'ancien score était\",score_XGB_2)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lznnJwj9zzti",
    "Q9xb4yREzztk",
    "esh6ztARzztk",
    "3rUl53Pqzztm",
    "t9PtxZDZzzt3"
   ],
   "name": "Apprent_Python_Visa_hide_nb_track.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
