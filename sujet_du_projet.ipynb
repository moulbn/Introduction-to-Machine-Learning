{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQ9PtbSBzztP"
   },
   "source": [
    "## Data preparation\n",
    "### Lecture \n",
    "The original dataset has been transformed after an important preprocessing step in this [notebook](https://github.com/wikistat/Exploration/blob/master/GRC-carte_Visa/Explo-R-Visa.ipynb). We skip this step today! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HLRhAQ5HzztQ"
   },
   "outputs": [],
   "source": [
    "# Library Importation.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BRSf5g7nzztT"
   },
   "outputs": [],
   "source": [
    "# Lecture of the data frame\n",
    "df = pd.read_table('vispremv.dat', delimiter=' ')\n",
    "#Give the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xRlqVxvrzztX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RELAT</th>\n",
       "      <th>AGER</th>\n",
       "      <th>OPGNBL</th>\n",
       "      <th>MOYRVL</th>\n",
       "      <th>TAVEPL</th>\n",
       "      <th>ENDETL</th>\n",
       "      <th>GAGETL</th>\n",
       "      <th>GAGECL</th>\n",
       "      <th>GAGEML</th>\n",
       "      <th>KVUNB</th>\n",
       "      <th>...</th>\n",
       "      <th>UEMNB</th>\n",
       "      <th>XLGNB</th>\n",
       "      <th>XLGMTL</th>\n",
       "      <th>YLVNB</th>\n",
       "      <th>YLVMTL</th>\n",
       "      <th>ROCNB</th>\n",
       "      <th>NPTAG</th>\n",
       "      <th>ITAVCL</th>\n",
       "      <th>HAVEFL</th>\n",
       "      <th>JNBJDL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>132.574788</td>\n",
       "      <td>42.573848</td>\n",
       "      <td>0.627211</td>\n",
       "      <td>2.371296</td>\n",
       "      <td>6.715418</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>4.477338</td>\n",
       "      <td>1.868891</td>\n",
       "      <td>2.394713</td>\n",
       "      <td>1.028222</td>\n",
       "      <td>...</td>\n",
       "      <td>1.468485</td>\n",
       "      <td>0.649106</td>\n",
       "      <td>4.161701</td>\n",
       "      <td>0.760113</td>\n",
       "      <td>4.726685</td>\n",
       "      <td>8.564440</td>\n",
       "      <td>0.136406</td>\n",
       "      <td>9.373976</td>\n",
       "      <td>3.043170</td>\n",
       "      <td>1.189791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>93.499830</td>\n",
       "      <td>11.912343</td>\n",
       "      <td>0.685315</td>\n",
       "      <td>1.464376</td>\n",
       "      <td>4.907295</td>\n",
       "      <td>1.279475</td>\n",
       "      <td>5.473534</td>\n",
       "      <td>3.680483</td>\n",
       "      <td>4.540024</td>\n",
       "      <td>0.501554</td>\n",
       "      <td>...</td>\n",
       "      <td>1.280262</td>\n",
       "      <td>0.794352</td>\n",
       "      <td>5.205502</td>\n",
       "      <td>0.782235</td>\n",
       "      <td>4.662683</td>\n",
       "      <td>11.738092</td>\n",
       "      <td>0.379837</td>\n",
       "      <td>3.313518</td>\n",
       "      <td>4.665580</td>\n",
       "      <td>1.587594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.067305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>130.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>8.652248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.882802</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.075969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>204.500000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>3.349750</td>\n",
       "      <td>10.962298</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>10.483833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.033572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.651494</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.649131</td>\n",
       "      <td>6.216606</td>\n",
       "      <td>2.441401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>393.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>3.367296</td>\n",
       "      <td>7.451242</td>\n",
       "      <td>13.785052</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>15.060512</td>\n",
       "      <td>13.123924</td>\n",
       "      <td>14.296852</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.270100</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.785052</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16.045468</td>\n",
       "      <td>15.770893</td>\n",
       "      <td>4.905275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             RELAT         AGER       OPGNBL       MOYRVL       TAVEPL  \\\n",
       "count  1063.000000  1063.000000  1063.000000  1063.000000  1063.000000   \n",
       "mean    132.574788    42.573848     0.627211     2.371296     6.715418   \n",
       "std      93.499830    11.912343     0.685315     1.464376     4.907295   \n",
       "min      -2.000000    18.000000     0.000000     0.000000     0.000000   \n",
       "25%      42.500000    33.000000     0.000000     1.386294     0.000000   \n",
       "50%     130.000000    43.000000     0.693147     2.564949     8.652248   \n",
       "75%     204.500000    52.000000     1.098612     3.349750    10.962298   \n",
       "max     393.000000    65.000000     3.367296     7.451242    13.785052   \n",
       "\n",
       "            ENDETL       GAGETL       GAGECL       GAGEML        KVUNB  ...  \\\n",
       "count  1063.000000  1063.000000  1063.000000  1063.000000  1063.000000  ...   \n",
       "mean      0.827903     4.477338     1.868891     2.394713     1.028222  ...   \n",
       "std       1.279475     5.473534     3.680483     4.540024     0.501554  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     1.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     1.000000  ...   \n",
       "75%       1.945910    10.483833     0.000000     0.000000     1.000000  ...   \n",
       "max       4.605170    15.060512    13.123924    14.296852     4.000000  ...   \n",
       "\n",
       "             UEMNB        XLGNB       XLGMTL        YLVNB       YLVMTL  \\\n",
       "count  1063.000000  1063.000000  1063.000000  1063.000000  1063.000000   \n",
       "mean      1.468485     0.649106     4.161701     0.760113     4.726685   \n",
       "std       1.280262     0.794352     5.205502     0.782235     4.662683   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     1.000000     4.882802   \n",
       "75%       2.000000     1.000000    10.033572     1.000000     9.651494   \n",
       "max       9.000000     4.000000    13.270100     4.000000    13.785052   \n",
       "\n",
       "             ROCNB        NPTAG       ITAVCL       HAVEFL       JNBJDL  \n",
       "count  1063.000000  1063.000000  1063.000000  1063.000000  1063.000000  \n",
       "mean      8.564440     0.136406     9.373976     3.043170     1.189791  \n",
       "std      11.738092     0.379837     3.313518     4.665580     1.587594  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     8.067305     0.000000     0.000000  \n",
       "50%       2.000000     0.000000    10.075969     0.000000     0.000000  \n",
       "75%      14.000000     0.000000    11.649131     6.216606     2.441401  \n",
       "max      69.000000     4.000000    16.045468    15.770893     4.905275  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produce a list of quantitative variables\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj4YpUKuzztY"
   },
   "source": [
    "Verify that most of the variables are described by a quantitative and a qualitative version.\n",
    "\n",
    "Qualitative variables  (sexe, csp, famille)  are transformed into quantitative ones except `CARVP`. How? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "skQOxgwEzztY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEXEQ      object\n",
       "FAMIQ      object\n",
       "PCSPQ      object\n",
       "kvunbq     object\n",
       "vienbq     object\n",
       "uemnbq     object\n",
       "xlgnbq     object\n",
       "ylvnbq     object\n",
       "rocnbq     object\n",
       "nptagq     object\n",
       "endetq     object\n",
       "gagetq     object\n",
       "facanq     object\n",
       "lgagtq     object\n",
       "havefq     object\n",
       "jnbjdq     object\n",
       "ageq       object\n",
       "relatq     object\n",
       "qsmoyq     object\n",
       "opgnbq     object\n",
       "moyrvq     object\n",
       "tavepq     object\n",
       "dmvtpq     object\n",
       "boppnq     object\n",
       "itavcq     object\n",
       "RELAT       int64\n",
       "AGER        int64\n",
       "OPGNBL    float64\n",
       "MOYRVL    float64\n",
       "TAVEPL    float64\n",
       "ENDETL    float64\n",
       "GAGETL    float64\n",
       "GAGECL    float64\n",
       "GAGEML    float64\n",
       "KVUNB       int64\n",
       "QSMOY       int64\n",
       "QCREDL    float64\n",
       "DMVTPL    float64\n",
       "BOPPNL    float64\n",
       "FACANL    float64\n",
       "LGAGTL    float64\n",
       "VIENB       int64\n",
       "VIEMTL    float64\n",
       "UEMNB       int64\n",
       "XLGNB       int64\n",
       "XLGMTL    float64\n",
       "YLVNB       int64\n",
       "YLVMTL    float64\n",
       "ROCNB       int64\n",
       "NPTAG       int64\n",
       "ITAVCL    float64\n",
       "HAVEFL    float64\n",
       "JNBJDL    float64\n",
       "CARVP      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vispremDum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9304/646431260.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvispremDum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SEXEQ\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvispremDum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvispremDum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SEXE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'vispremDum' is not defined"
     ]
    }
   ],
   "source": [
    "vispremDum[\"SEXEQ\"] = vispremDum.loc[:, vispremDum.columns.str.startswith(\"SEXE\")].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end of nvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Xa1IRj-4zztZ"
   },
   "outputs": [],
   "source": [
    "# Transformation of qualitative variables into quantitative 0/1 ones.\n",
    "vispremDum = pd.get_dummies(df[[\"SEXEQ\", \"FAMIQ\", \"PCSPQ\"]])\n",
    "vispremDum.drop([\"SEXEQ_Sfem\", \"FAMIQ_Fcou\"], axis=1, inplace=True)\n",
    "\n",
    "# Aggregation of the previous variables with numeric ones\n",
    "vispremNum = vispremDum.select_dtypes(exclude = ['object'])\n",
    "vispremR = pd.concat([vispremDum, vispremNum], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA5DcEW8zzta"
   },
   "source": [
    "**Q** How many samples and how many variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wMMDekUAzzta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of samples : 1063 \n",
      " Number of variables : 14\n"
     ]
    }
   ],
   "source": [
    "#TODO Compute the number of samples and the number of variables\n",
    "print(f\" Number of samples : {vispremR.shape[0]} \\n Number of variables : {vispremR.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ibjHCLbPzztb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148009     1\n",
       "552427     1\n",
       "556005     1\n",
       "556686     1\n",
       "642680     1\n",
       "          ..\n",
       "7567726    0\n",
       "7572458    0\n",
       "7574479    0\n",
       "7580358    0\n",
       "7589439    0\n",
       "Name: CARVP, Length: 1063, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The variable CARVP is modified into 0/1\n",
    "df[\"CARVP\"] = df[\"CARVP\"].map(lambda x: 0 if x==\"Cnon\" else 1)\n",
    "df[\"CARVP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GChp_bI-zztb"
   },
   "source": [
    "### Extraction of the learning and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "yJxen2nLzztb"
   },
   "outputs": [],
   "source": [
    "rd_seed = 123 # Modify this sieve initialization\n",
    "npop = len(df)\n",
    "\n",
    "#TODO Find xApp,xTest,yApp,yTest with the help of train_test_split\n",
    "#We will place 200 samples in the test set\n",
    "\n",
    "x_vars = vispremR\n",
    "y_var = df[[\"CARVP\"]]\n",
    "xApp, xTest, yApp, yTest = train_test_split(x_vars, y_var, test_size = 200, random_state = rd_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cpamcf7Szztc"
   },
   "source": [
    "## [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "This \"old\" method is still commonly used, mainly because of its simplicity, interpretability and diffusion over years.\n",
    "\n",
    "### Estimation and optimization\n",
    "The model selection procedure involves a statistical penalization: *ridge*, Lasso or a combination of both (*elastic net*). The behaviour of the model selection in Python is not exactly comparable with the one used in R (*stepwise, backward, forward*) that optimizes *AIC*\n",
    "\n",
    "#### *Lasso* Optimization\n",
    "\n",
    "\n",
    "We first consider the **Lasso** penalty, which is proportional to the L1 norm of the parameter we are looking for. This penalty induces sparsity of the response coefficient vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y into a 1D array\n",
    "yApp = np.array(yApp[\"CARVP\"])\n",
    "yTest = np.array(yTest[\"CARVP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "HQq_FBurzztc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Grid of penalty parameters\n",
    "param = [{\"C\": [0.5, 1, 5, 10, 12, 15, 30]}] # dictionary of the values of C used for GridSearchCV\n",
    "# TODO\n",
    "# 1. Define a model (logistic regression)\n",
    "# 2. Use the penalty=\"l1\"\n",
    "# 3. Use GridSearchCV to obtain automatically the best regularization parameter \n",
    "# 4. In what follows, this method will be denoted by logitLasso !\n",
    "\n",
    "logit_model = LogisticRegression(penalty=\"l1\", solver=\"liblinear\")\n",
    "\n",
    "grid_search_cv = GridSearchCV(logit_model, param, cv=5)\n",
    "\n",
    "logitLasso = grid_search_cv.fit(xApp, yApp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "W8lxGldIzztd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate = 0.231758, Best parameter  = {'C': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Computation of the loss\n",
    "print(\"Best learning rate = %f, Best parameter  = %s\" %\n",
    "      (1.-logitLasso.best_score_, logitLasso.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTpjmh6Hzzte"
   },
   "source": [
    "Prediction error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "XA_eOwnxzzte"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction    0   1\n",
      "true value         \n",
      "0           105  29\n",
      "1            30  36 \n",
      "\n",
      "Test error - Logistic regression  Lasso = 0.295000\n"
     ]
    }
   ],
   "source": [
    "# Prediction with the model\n",
    "yChap = logitLasso.predict(xTest)\n",
    "# TODO:  compute the  confusion matrix with the help of pd.crosstab\n",
    "\n",
    "confusion_matrix = pd.crosstab(yTest, yChap, colnames=[\"prediction\"], rownames=[\"true value\"])\n",
    "print(confusion_matrix, \"\\n\")\n",
    "\n",
    "# Error on the test set\n",
    "print(\"Test error - Logistic regression  Lasso = %f\" % (1-logitLasso.score(xTest, yTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo_8DuT4zztf"
   },
   "source": [
    "#### *Ridge* Optimization \n",
    "On considère maintenant l'optimisation Ridge, ou la pénalité est proportionnelle à la norme 2 de l'estimateur (au carré)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "hTQgjc3Vzztf"
   },
   "outputs": [],
   "source": [
    "# Grid of penalty parameters\n",
    "param=[{\"C\":[0.5,1,5,10,12,15,30]}]\n",
    "# TODO\n",
    "# 1. Define a model (logistic regression)\n",
    "# 2. Use the penalty=\"l2\"\n",
    "# 3. Use GridSearchCV to obtain automatically the best regularization parameter \n",
    "# 4. In what follows, this method will be denoted by logitRidge!\n",
    "\n",
    "logit_model_ridge = LogisticRegression(penalty=\"l2\")\n",
    "\n",
    "gs_cv_ridge= GridSearchCV(logit_model_ridge, param, cv=5)\n",
    "\n",
    "logitRidge = gs_cv_ridge.fit(xApp, yApp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "vwoWpqcizztg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score = 0.231758, Best parameter = {'C': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Error computation  \n",
    "print(\"Best score = %f, Best parameter = %s\" %\n",
    "      (1. - logitRidge.best_score_, logitRidge.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "8RHd7XIuzztg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction    0   1\n",
      "true value         \n",
      "0           105  29\n",
      "1            30  36 \n",
      "\n",
      "Erreur de test régression Ridge = 0.295000\n"
     ]
    }
   ],
   "source": [
    "# Prediction \n",
    "yChap_ridge = logitRidge.predict(xTest)\n",
    "\n",
    "#Todo confusion matrix \n",
    "\n",
    "confusion_matrix_ridge = pd.crosstab(yTest, yChap_ridge, colnames=[\"prediction\"], rownames=[\"true value\"])\n",
    "print(confusion_matrix_ridge, \"\\n\")\n",
    "\n",
    "# Error on the test set\n",
    "print(\"Erreur de test régression Ridge = %f\" % (1-logitRidge.score(xTest, yTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rGWzM9Tzztg"
   },
   "source": [
    "**Q** Note the prediction errors and compare them with the ones predicted by the cross validation step.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "LassoOpt produced by GridSearchCV does not record the values of the parameters learnt by the model. It is then necessary to launch another time this model with the optimal value of the parameter if we wish to show the values of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZfbh3Gpzzth"
   },
   "outputs": [],
   "source": [
    "LassoOpt=LogisticRegression(penalty=\"l1\",C=12)\n",
    "LassoOpt=LassoOpt.fit(xApp, yApp)\n",
    "# Storage of the coefficients\n",
    "vect_coef=np.matrix.transpose(LassoOpt.coef_)\n",
    "vect_coef=vect_coef.ravel()\n",
    "#Show the most important 25 coefficients\n",
    "coef=pd.Series(abs(vect_coef),index=xApp.columns).sort_values(ascending=False)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMTUYzrOzzth"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "coef.plot(kind='bar')\n",
    "plt.title('Coeffients')\n",
    "plt.tight_layout()\n",
    "send(plt,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plmejigrzzth"
   },
   "source": [
    "**Q** What are the important variables? How to draw some conclusion/interpretation?\n",
    "\n",
    "**Q** Is the Lasso penalty efficient?\n",
    "\n",
    "It would be interesting to compare with the *ridge* and *elastic net* model\n",
    "\n",
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhe8trCDzzth"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "listMethod=[[\"Lasso\",logitLasso],[\"Ridge\",logitRidge]]\n",
    "\n",
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].predict_proba(xTest)\n",
    "    fpr, tpr, thresholds = roc_curve(yTest, probas_[:,1])\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc=\"best\")\n",
    "send(plt,9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf7KpbQnzzti"
   },
   "source": [
    "## Discriminant analysis\n",
    "Three methods are availables: parametric ones with LDA-QDA and a non parametric one (*k* nearest neighbor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0yl_Rctzzti"
   },
   "outputs": [],
   "source": [
    "from sklearn import discriminant_analysis\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lznnJwj9zzti"
   },
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "Estimation of the model (there is no feature selection step) and then prediction over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmcjK1fzzztj"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Define the model ('lda' as discriminant_analysis.LinearDiscriminantAnalysis)\n",
    "# 2. Fit the model\n",
    "# 3. Predict on the test set\n",
    "# 4. Compute the confusion matrix\n",
    " \n",
    "\n",
    "\n",
    "# Prediction on the test set\n",
    "print(\"Erreur de test lda = %f\" % (1-disLin.score(xTest,yTest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9xb4yREzztk"
   },
   "source": [
    "**Q** What about the quality of the prediction? The ability of interpret the method?\n",
    "\n",
    "**Q** What is the meaning of the  *warning*? What are the variables involved by this warning?\n",
    "### Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW-C5zF2zztk"
   },
   "outputs": [],
   "source": [
    "# Same procedure as the one of LDA for QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esh6ztARzztk"
   },
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrt8aIBhzztk"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Define the model with the 10 nearest neighbors (use KNeighborsClassifier)\n",
    "# 2. Fit the model\n",
    "# 3. Predict on the test set\n",
    "# 4. Show the confusion matrix \n",
    "\n",
    "print(table)\n",
    "# Prediction error on the test set\n",
    "print(\"Erreur de test knn = %f\" % (1-disKnn.score(xTest,yTest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzB3aBs3zztl"
   },
   "outputs": [],
   "source": [
    "#Optimization of the smoothing parameter (number of neighbors) k\n",
    "#Grid \n",
    "param_grid=[{\"n_neighbors\":list(range(1,15))}]\n",
    "disKnn=GridSearchCV(KNeighborsClassifier(),param_grid,cv=5,n_jobs=-1)\n",
    "disKnnOpt=disKnn.fit(xApp, yApp) # GridSearchCV is itself an estimator \n",
    "# Optimal parameter \n",
    "disKnnOpt.best_params_[\"n_neighbors\"]\n",
    "print(\"Best score = %f, Best parameter = %s\" % (1.-disKnnOpt.best_score_,disKnnOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDRONWXpzztl"
   },
   "outputs": [],
   "source": [
    "# Prediction on the test set \n",
    "yChap = disKnnOpt.predict(xTest)\n",
    "# Confusion matrix \n",
    "table=pd.crosstab(yChap,yTest)\n",
    "print(table)\n",
    "\n",
    "# Estimation of the prediction error on the test set \n",
    "print(\"Error rate of knn_opt = %f\" % (1-disKnnOpt.score(xTest,yTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg1mHDKczztm"
   },
   "source": [
    "ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xk2C2-Sbzztm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "# List of the methods \n",
    "listMethod=[[\"lda\",disLin],[\"qda\",disQua],[\"knn\",disKnnOpt]]\n",
    "# Curves computation\n",
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].predict_proba(xTest)\n",
    "    fpr, tpr, thresholds = roc_curve(yTest, probas_[:,1])\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rUl53Pqzztm"
   },
   "source": [
    "## [Binary decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "Binary trees are competing well versu logistic regression and are commonly used in datascience. In particular, their interpretation are simple, which is a great advantage of this method. However, the optimization of the parameters involved in this method is somewhat versatile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lcv6a0rZzztm"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntKX4A6Szztn"
   },
   "outputs": [],
   "source": [
    "# Define and fit the model\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoyRkvqSzztn"
   },
   "source": [
    "**Q** What is the default homogeneity criterion used by this method?\n",
    "\n",
    "**Q** What is the major drawback of the pruning step in  `Scikkit-learn` when compared to the  `rpart` library in R?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDx159w-zzt0"
   },
   "outputs": [],
   "source": [
    "# Optimization of the depth of the tree\n",
    "#TODO\n",
    "\n",
    "# optimal parameter\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (1. - treeOpt.best_score_,treeOpt.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldrincNzzzt1"
   },
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "yChap = treeOpt.predict(xTest)\n",
    "# Confusion matrix\n",
    "table=pd.crosstab(yChap,yTest)\n",
    "print(table)# Prediction error on the test set\n",
    "print(\" Prediction error on the test set = %f\" % (1-treeOpt.score(xTest,yTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXZLzvQJzzt2"
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydotplus\n",
    "treeG=DecisionTreeClassifier(max_depth=treeOpt.best_params_['max_depth'])\n",
    "treeG.fit(xApp,yApp)\n",
    "dot_data = StringIO() \n",
    "export_graphviz(treeG, out_file=dot_data) \n",
    "graph=pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_png(\"treeOpt.png\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXLc7AHEzzt2"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='treeOpt.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3yDHBt7zzt2"
   },
   "source": [
    "### [Roc curve ROC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "Comparison of the previous methods\n",
    "\n",
    "The default threshold   (0.5) is not necessarily the best one, and it is necessary to compare the ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7vBaFrVzzt2"
   },
   "outputs": [],
   "source": [
    "# Liste of the  methods \n",
    "listMethod=[[\"Logit\",logitLasso],[\"lda\",disLin],[\"Arbre\",treeOpt]]\n",
    "# Roc curves \n",
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].predict_proba(xTest)\n",
    "    fpr, tpr, thresholds = roc_curve(yTest, probas_[:,1])\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ww3nsy5zzt3"
   },
   "source": [
    "Comment the results.\n",
    "\n",
    "**Q** Interest of the logistic regression when compared to the LDA?\n",
    "\n",
    "**Q** Consequence of the ROC curve crossing on the AUC evaluation?\n",
    "\n",
    "The size of the test set (200) is modest..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9PtxZDZzzt3"
   },
   "source": [
    "## [Aggregation methods](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "We mainly compare here the three algorthms: *bagging, random forest, boosting*.\n",
    "\n",
    "### *Bagging*\n",
    "\n",
    "**Q** What is the default aggregated algorithm? \n",
    "\n",
    "**Q** What is the default number of estimators ? Is it necessary to optimize this number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbJ2YHmUzzt3"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag= # Def model (nestim =100)\n",
    "bagC=bag.fit(xApp, yApp)\n",
    "# Prediction on the test set\n",
    "yChap = bagC.predict(xTest)\n",
    "# Confusion matrix\n",
    "table=pd.crosstab(yChap,yTest)\n",
    "print(table)\n",
    "\n",
    "# Prediction error on the test set\n",
    "print(\"Prediction error with bagging = %f\" % (1-bagC.score(xTest,yTest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRv9qorYzzt4"
   },
   "source": [
    "**Q** Run this previous cell several times. What do you think about the stability of the method and about the its reliability?\n",
    "\n",
    "### *Random forest*\n",
    "\n",
    "**Q** What is the parameter to be optimized for this algorithm? What is its default value?\n",
    "\n",
    "**Q** Is the number of trees a versatile parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui8diWdxzzt4"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtjrU0tozzt4"
   },
   "outputs": [],
   "source": [
    "# Optimization  of max_features\n",
    "param=[{\"max_features\":list(range(2,10,1))}]\n",
    "\n",
    "rf= # TODO DEF gridsearchCV\n",
    "rfOpt=rf.fit(xApp, yApp)\n",
    "# optimal parameter \n",
    "print(\"Best score = %f, Best parameter = %s\" % (1. - rfOpt.best_score_,rfOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4Zz1Ffszzt4"
   },
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "yChap = rfOpt.predict(xTest)\n",
    "# Confusion matrix\n",
    "table=pd.crosstab(yChap,yTest)\n",
    "print(table)\n",
    "\n",
    "# Prediction error on the test set\n",
    "print(\"Prediction error with  random forest opt -quantitative = %f\" % (1-rfOpt.score(xTest,yTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnLJfrlczzt4"
   },
   "source": [
    "### *Gradient boosting*\n",
    "\n",
    "**Q** What is the historical *boosting* algorithm? Which one is used now?\n",
    "\n",
    "**Q** What are the important parameters to be tuned? How to calibrate them??\n",
    "\n",
    "**Q** What is the default value of the parameter that is not optimized below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSKMtz23zzt5"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Optimization of two parameters\n",
    "paramGrid = [\n",
    "  {'n_estimators': list(range(100,601,50)), 'learning_rate': [0.1,0.2,0.3,0.4]}\n",
    " ]\n",
    "gbmC= GridSearchCV(GradientBoostingClassifier(),paramGrid,cv=5,n_jobs=-1)\n",
    "gbmOpt=gbmC.fit(xApp, yApp)\n",
    "# Optimal parameters\n",
    "print(\"Best score = %f, Best parameters = %s\" % (1. - gbmOpt.best_score_,gbmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSk8XVkCzzt5"
   },
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "yChap = gbmOpt.predict(xTest)\n",
    "# Confusion matrix\n",
    "table=pd.crosstab(yChap,yTest)\n",
    "print(table)\n",
    "\n",
    "# Prediction error on the test set\n",
    "print(\"Test error of gbm opt = %f\" % (1-gbmOpt.score(xTest,yTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXT7rtALzzt5"
   },
   "source": [
    "### Courbes ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y9fQ5R_zzt5"
   },
   "outputs": [],
   "source": [
    "# List of the methods\n",
    "listMethod=[[\"Logit\",logitLasso],[\"lda\",disLin],[\"Arbre\",treeOpt],[\"RF\",rfOpt],[\"GBM\",gbmOpt]]\n",
    "# ROC curves computation \n",
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].predict_proba(xTest)\n",
    "    fpr, tpr, thresholds = roc_curve(yTest, probas_[:,1])\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate ')\n",
    "plt.legend(loc=\"best\")\n",
    "send(plt,17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5Tsoaj_zzt6"
   },
   "source": [
    "**Q** What is the best interpretable method? What is the best method?\n",
    "\n",
    "**Q** What can you say about the *extrem gradient boosting* ? Number of parameters to be tuned? In Python? In R? Its diffusion?\n",
    "\n",
    "**Exercice** Add the deep learning and SVM family of methods."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Apprent-Python-Visa_hide-nb-track.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
