{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hy7-_IL_Eou"
   },
   "source": [
    "# Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icT_zq0r_Eov"
   },
   "source": [
    "### 1) \n",
    "\n",
    "We hereby consider a two-category classification problem with a general framework of Machine Learning pipeline. \n",
    "The \"best\" solution to a classification problem minimizes the risk over all possible classifiers, according to a designated loss function, and is called $f^*$. To that extent, a two-category classifier usually relates to the $0-1$ Loss function i.e.: \n",
    "\n",
    "$f^* = +1$ $\\iff$  $P(Y=1| X ) \\geq P(Y=0| X ) $\n",
    "$ \\iff P(Y=1| X ) \\geq 1/2 $ \n",
    "\n",
    "and \n",
    "\n",
    "$f^* = 0$ otherwise\n",
    "\n",
    "\n",
    "\n",
    "Plugging in the priors into the above formula, the Bayes Rule states that \n",
    "\n",
    "for $(X_i, Y_i)_{1 \\leq i \\leq N})$ such that $X \\in R^P$ and $Y \\in R$,\n",
    "\n",
    "a training set $D_n = { (X_1, Y_1), ..., (X_n, Y_n)}$ i.i.d according to $P$,\n",
    "\n",
    "the best solution $f^*$, predictor function in $F : { X(\\Omega) --> Y(\\Omega) a.s } $ yields \n",
    "\n",
    "$f^* (X) = +1$ $\\iff$ $P(Y=1| X=x)$ $= \\frac{{\\pi_1}{f_1}(x)}{\\sum \\limits_{l=0}^{1}{{\\pi_l}{f_l}}(x) } $   $\\geq \\frac{{\\pi_0}{f_0}(x)}{\\sum \\limits_{l=0}^{1}{{\\pi_l}{f_l}}(x) } = P(Y=0| X=x) $   \n",
    "\n",
    "and \n",
    "\n",
    "$f^* (X) = 0$ otherwise\n",
    "   $\\space$ $\\space$ $\\space$ $\\space$ $\\space$  $\\space$ $\\space$\n",
    "\n",
    " Indeed, one can further simply the above equation and rewrite: \n",
    "\n",
    " $f^* = +1$ $\\iff$ $P(Y=1| X=x)$ $\\geq 1/2 $ \n",
    " \n",
    " and \n",
    " \n",
    " $f^* = 0$ otherwise\n",
    "\n",
    "\n",
    " since the denominator is strictly positive. \n",
    "\n",
    "\n",
    "Moreover, the Mean Classification Risk simply equals the expectation of the loss function i.e. the $0-1$ loss obtained by the classifier pipeline by misclassfying observations. The Risk equals \n",
    "\n",
    "$R(f) = P(Y \\neq f(X) ) =  P(Y =1,  f(X) = 0 ) +  P(Y = 0 , f(X) = 1 ) $\n",
    "\n",
    "\n",
    "Thus, the Mean Classification Risk yields: \n",
    "\n",
    "$E[R(f)] = \\sum_{f \\in F} \\mathbb{1}_{Y \\neq f(X)} * P(Y \\neq f(X) ) $\n",
    "\n",
    "### 2)\n",
    "\n",
    "$ \\forall f \\in F, $\n",
    "\n",
    "$ R(f) - R(f^*) = \\int_{f \\neq f^*}{} | 2P(Y=1| X=x) - 1| P_x (dx) = E_X [|2P(Y=1| X=x) - 1| \\mathbb{1}_{f(X) \\neq f^*(X)} $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$Proof.$\n",
    "\n",
    "$ R(f) = P(Y \\neq f(X) ) =  P(Y =1,  f(X) = 0 ) +  P(Y = 0 , f(X) = 1 ) $\n",
    "\n",
    "so that using the Law of Iterated Expectations on the disjoints events, \n",
    "\n",
    "$E[E[ \\mathbb{1}_{Y = 1, f(X) = 0 |X}]] + E[E[ \\mathbb{1}_{Y = 0, f(X) = 1 |X}]]  $\n",
    "\n",
    "since $f(X)$ is measurable with respect to $X$, we can factor it out and yield:\n",
    "\n",
    "$ E [ \\mathbb{1}_{f(X)=0} P(Y =1,  f(X) = 0 ) + \\mathbb{1}_{f(X)=1} (1- P(Y =1,  f(X) = 0 ) )  $\n",
    "\n",
    "if $f= f^*$, \n",
    "\n",
    "$ E [ \\mathbb{1}_{P(Y =1,  f(X) = 0 ) \\leq 1/2} P(Y =1,  f(X) = 0 ) + \\mathbb{1}_{P(Y =1,  f(X) = 0 ) > 1/2} (1- P(Y =1,  f(X) = 0 ) )  $\n",
    "\n",
    "$\\iff $\n",
    "\n",
    "$R(f^*) = E [ (\\mathbb{1}_{P(Y =1,  f(X) = 0 ) \\leq 1/2} + \\mathbb{1}_{P(Y =1,  f(X) = 0 ) > 1/2} ) min(P(Y =1,  f(X) = 0 ),(1- P(Y =1,  f(X) = 0 ) ) )$\n",
    "\n",
    "\n",
    "$\\iff $\n",
    "\n",
    "$R(f^*) = E[min(P(Y =1,  f(X) = 0 ),(1- P(Y =1,  f(X) = 0 ) ) ) \\leq 1/2 $\n",
    "\n",
    "\n",
    " since\n",
    " \n",
    "  $min(P(Y =1,  f(X) = 0 ),(1- P(Y =1,  f(X) = 0 ) ) ) \\leq 1/2 $\n",
    "\n",
    "  Now, given an arbitrary classifier $f$, one gets\n",
    "\n",
    "  $R(f) - R(f^*) = E [ \\mathbb{1}_{f(X)=0} P(Y =1,  f(X) = 0 ) + \\mathbb{1}_{f(X)=1} (1- P(Y =1,  f(X) = 0 ) ) - \\mathbb{1}_{f^*(X)=0} P(Y =1,  f^*(X) = 0 ) + \\mathbb{1}_{f^*(X)=1} (1- P(Y =1,  f^*(X) = 0 ) ) ] $\n",
    "\n",
    "  $\\iff $\n",
    "\n",
    "  $  E [ (\\mathbb{1}_{f(X)=0} - \\mathbb{1}_{f^*(X)=0} )P(Y=1| X=x) + (\\mathbb{1}_{f(X)=1} - \\mathbb{1}_{f^*(X)=1} )(1 - P(Y=1| X=x) )  $ \n",
    "\n",
    "  which in turn yields \n",
    "\n",
    "  $ E_X [(2P(Y=1| X=x) - 1) (f(X) = 0) -  \\mathbb{1}_{f^*(X) = 0} ] $\n",
    "\n",
    "\n",
    "### 3)\n",
    "\n",
    "Linear Discriminant Analysis was first proposed by Fisher in 1936 as a method to predict (well separated) $k$ classes, in response to logistic regression model's instabilities. We hereby study the case of a two-category classification problem i.e., card-ownerniship or no. Using Bayes' Theorem to derive the best classifier, i.e., with the lowest possible $total$ error rate out of all classifiers (provided that the Gaussian model is correct): \n",
    "\n",
    "$P(Y=1| X=x)$ $= \\frac{{\\pi_k}{f_k}(x)}{\\sum \\limits_{l=1}^{K}{{\\pi_l}{f_l}}(x) } $ $ \\forall k = $  {$0;1$}      $\\space$ $\\space$ $\\space$ $\\space$ $\\space$  $\\space$ $\\space$ ($1.1$)\n",
    "\n",
    "\n",
    "such that \n",
    "\n",
    "${\\pi_k}$ being the prior probability that a random chosen observation belongs to the class of card-owners coded as $k=1$ \n",
    "\n",
    "${f_k}(X)= P(X=x| Y=k)$  being the conditional density function of $X$ for an observation belonging to the $k^{th}$ class $ \\forall k = $  {$0;1$}\n",
    "\n",
    "\n",
    "We thus estimate prior ${\\pi_k}$ and the conditional probability density function ${{f_k}}(X)$ by $\\hat{\\pi_k}$ and  $\\hat{{f_k}}(X)$ respectively in order to plug them in the posterior $(1.1)$. \n",
    "\n",
    "We assume that the predictors $X$ are normally distributed according to a $N(\\mu_k, \\Sigma)$ with a class-specific mean-vector of size ($k$x$1$) and homoskedastic, that is, there is a common covariance matrix $\\Sigma$ of size ($p$x$p$) across both classes. We plug the resulting multivariate gaussian distribution function in $(1)$ as the $\\hat{{f_k}}(X)$.\n",
    "The unbiased maximum likelihood estimates  $\\hat\\mu_k$ and $\\hat\\sigma^2$ for $\\mu_k$ and $\\sigma^2$ are used to compute ${{f_k}}(X)$ $ \\forall k, \\space \\space  \\forall p  $: \n",
    "\n",
    "\n",
    "$\\hat\\mu_{k,p} = \\frac{1}{n_k}\\sum \\limits_{i:y_i =k}{}x_{i,p}$ \n",
    "\n",
    "$\\hat\\Sigma = \\frac{1}{n_K}(x_i - \\hat\\mu_k)^T(x_i - \\hat\\mu_k)\n",
    "  \\forall k = $  {$0;1$} $ \\space \\space \\space \\space \\space  \\space \\space (1.2)$\n",
    "\n",
    "\n",
    "Naturally, $\\hat{\\pi_k}$ is equal to the empirical sample share of individual belonging to class $k$:\n",
    "\n",
    "\n",
    "$\\hat{\\pi_k}$ = $\\frac{n_k}{n} \\forall k = $  {$0;1$} $\\space$ $\\space$ $\\space$ $\\space$ $\\space$  $\\space$ $\\space$ ($1.3$)\n",
    "\n",
    "\n",
    "The Bayes classifier thus assigns an observation $X=x$ to the class for which the estimate $\\hat{P}(Y=1| X=x)$ is largest (i.e. greater than $1/2$).\n",
    "Taking the log of ($1.1$) with $\\hat{{f_k}}(X)$ and $\\hat{\\pi_k}$ and simplifying the constant terms yields the following Bayes decision boundary rule: \n",
    "\n",
    "\"$Assign$ $y_i$ $to$ $k=1$ $\\iff$ $2x^T(\\mu_1 - \\mu_2) + log(\\pi_1) - log(\\pi_2) \\ge {\\mu_1}^T{\\mu_1} - {\\mu_2}^T{\\mu_2}$ and to $k=0$ otherwise.\"  ($1.4$)\n",
    "\n",
    "Which is linear in $X$.\n",
    "\n",
    "\n",
    "The Bayes decision boundary thus corresponds to the point where \n",
    "$ x= 2(\\mu_1 - \\mu_2)^{-1}({\\mu_1}^T{\\mu_1} - {\\mu_2}^T{\\mu_2} -log(\\frac{\\pi_1}{\\pi_2})) = \\frac{1}{2}[({\\mu_1 + \\mu_2})-(\\mu_1 - \\mu_2)^{-1}({log(\\frac{\\pi_1}{\\pi_2})})]$ $\\space$ $\\space$ $\\space$ $\\space$ $\\space$  $\\space$ $\\space$ ($1.5$)\n",
    "\n",
    "\n",
    "\n",
    "The $LDA$ classifier plugs the estimates given in $(1.2)$ and $(1.3)$ into ($1.4$) in order to assign an observation $X=x$ to the most probable class.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Q9xb4yREzztk",
    "esh6ztARzztk",
    "3rUl53Pqzztm",
    "t9PtxZDZzzt3"
   ],
   "name": "Apprent_Python_Visa_hide_nb_track (1) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
